{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01measyocr\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch \n",
    "import easyocr\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_ocr(image_path, yolo_model_path, ocr_languages=['en']):\n",
    "    # Load YOLO model\n",
    "    model = YOLO(yolo_model_path)\n",
    "    \n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Perform object detection\n",
    "    results = model(image)\n",
    "    \n",
    "    # Initialize OCR reader\n",
    "    reader = easyocr.Reader(ocr_languages)\n",
    "    \n",
    "    # Process detected objects\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        \n",
    "        for box in boxes:\n",
    "            # Get bounding box coordinates\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            \n",
    "            # Crop detected object\n",
    "            cropped_obj = image[y1:y2, x1:x2]\n",
    "            \n",
    "            # Perform OCR on cropped object\n",
    "            ocr_results = reader.readtext(cropped_obj)\n",
    "            \n",
    "            # Draw bounding box and detected text\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            for (bbox, text, score) in ocr_results:\n",
    "                print(f\"Detected Text: {text} (Score: {score})\")\n",
    "                cv2.putText(image, text, (x1, y1-10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "    \n",
    "    # Save result image\n",
    "    cv2.imwrite('detected_image.jpg', image)\n",
    "    return image\n",
    "\n",
    "# Example usage\n",
    "detect_and_ocr(\n",
    "    image_path='your_annotated_image.jpg', \n",
    "    yolo_model_path='yolov8n.pt'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
